{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyenne/mycv_docs/blob/main/NOP's_Basic_Staple_Diffusion_v0_52_(1_4_Weights).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "NOP's Basic Staple Diffusion v0.52 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stablity.AI Model Terms of Use\n",
        "By using this Notebook, you agree to the following Terms of Use, and license\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "\n",
        "You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "\n",
        "CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "\n",
        "You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license"
      ],
      "metadata": {
        "id": "fSanMIbKMydc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Donate"
      ],
      "metadata": {
        "id": "h-qNQtzBRbjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally did not want to include this, but people keep on asking me if they could donate. So if you want to fuel my caffeeine addiction: https://www.buymeacoffee.com/NOP1337 . 100% of the donations will go towards coffee\n",
        "\n",
        "Don't feel obligated to, this will always remain free no matter what"
      ],
      "metadata": {
        "id": "PmIPmy4jRegy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5V9MWbFtpNi"
      },
      "source": [
        "#GPU Info\n",
        " (If it throws an error here, go to Runtime, then click \"Change Runtime Type\" and then select \"GPU\"). There's also a chance that Colab put you on a GPU timeout if this is set and it still throws an error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ekR-LW6trWG"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHaZZ0uKti1c"
      },
      "source": [
        "# Diffusers Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucr5_i21xSjv"
      },
      "outputs": [],
      "source": [
        "#@title Render Images\n",
        "class Cleaner:\n",
        "  def clean_env():\n",
        "    gc, torch = Manager.manage_imports('clean_env')\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "class Colab:\n",
        "  def __init__(self):\n",
        "    self.settings = self.UserSettings.set_settings()\n",
        "\n",
        "  def clear():\n",
        "    from IPython.display import clear_output; clear_output()\n",
        "\n",
        "  def manage_drive(drive_pic_dir):\n",
        "    exists, mount, makedirs = Manager.manage_imports('manage_drive')\n",
        "    if not exists('/content/drive'):\n",
        "      mount('/content/drive')\n",
        "    if not exists(f'/content/drive/MyDrive/{drive_pic_dir}'):\n",
        "      makedirs(f'/content/drive/MyDrive/{drive_pic_dir}')\n",
        "\n",
        "  class Images:\n",
        "    def resize_image():\n",
        "      pass\n",
        "\n",
        "    def suggest_resolution():\n",
        "      pass\n",
        "      \n",
        "    class Painter:\n",
        "      def inpaint(width, height):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        def draw(filename='drawing.png', color=\"white\", w=256, h=256, line_width=50,loop=False, init_img=\"init.jpg\"):\n",
        "          filename=\"init.jpg\"\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"100\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "        }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width};\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          \n",
        "          var button = document.querySelector('#save')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          reload_img_button.click()\n",
        "        \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "\n",
        "              var c = ctx\n",
        "              var imageData = ctx.getImageData(0,0, {w}, {h});\n",
        "              var pixel = imageData.data;\n",
        "              var r=0, g=1, b=2,a=3;\n",
        "            for (var p = 0; p<pixel.length; p+=4)\n",
        "            {{\n",
        "              if (\n",
        "                  pixel[p+r] != 255 &&\n",
        "                  pixel[p+g] != 255 &&\n",
        "                  pixel[p+b] != 255) \n",
        "              {{pixel[p+r] =0; pixel[p+g]=0; pixel[p+b]=0}}\n",
        "            }}\n",
        "\n",
        "            c.putImageData(imageData,0,0);\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(\"init_mask.png\", 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "\n",
        "\n",
        "\n",
        "        draw(filename = \"init_mask.png\", w=width, h=height)\n",
        "        import PIL.Image\n",
        "        return PIL.Image.open('init_mask.png')\n",
        "      def img2img(width, height):\n",
        "        import os\n",
        "        os.chdir('/content/')\n",
        "        def draw(filename='drawing.png', color=\"black\", bg_color=\"transparent\",w=256, h=256, line_width=1,loop=False):\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "        <div>\n",
        "          <label for=\"strokeColor\">Stroke</label>\n",
        "          <input type=\"color\" value=\"{color}\" id=\"strokeColor\">\n",
        "        \n",
        "          <label for=\"bgColor\">Background</label>\n",
        "          <input type=\"color\" value=\"{bg_color}\" id=\"bgColor\">\n",
        "        </div>\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"35\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "          <button id=\"exit\">Exit</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "      }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width}\n",
        "          ctx.fillStyle = \"{bg_color}\";\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "          var strokeColor = document.querySelector('#strokeColor')\n",
        "          var bgColor = document.querySelector('#bgColor')\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          bgColor.addEventListener(\"change\", updateBG, false);\n",
        "          strokeColor.addEventListener(\"change\", updateStroke, false);\n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          var button = document.querySelector('#save')\n",
        "          var exit_button = document.querySelector('#exit')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            exit_button.onclick = ()=>{{\n",
        "            resolve()\n",
        "          }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          // window.onload = async ()=>{{\n",
        "          //   console.log(\"loaded\")\n",
        "          //   let img = await loadImage('{html_filename}');  \n",
        "          //   ctx.drawImage(img, 0, 0);\n",
        "          // }}\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(filename, 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "        \n",
        "        draw(filename = \"custom_image.png\", w=width, h=height, bg_color=\"blue\", line_width=10)\n",
        "        import PIL.Image\n",
        "        return PIL.Image.open(\"/content/custom_image.png\")\n",
        "\n",
        "  class UserSettings:\n",
        "\n",
        "    def set_settings():\n",
        "      MODE = \"PROMPT\" \n",
        "\n",
        "      settings = {\"mode\":MODE}\n",
        "\n",
        "      WIDTH = 512 \n",
        "      HEIGHT = 512 \n",
        "      SCALE = 13.8 \n",
        "      SEED = 0 \n",
        "      settings[\"seed\"] = SEED \n",
        "      SCHEDULER = 'default' \n",
        "      settings[\"scheduler\"] = SCHEDULER \n",
        "\n",
        "      if SCHEDULER == 'ddim':\n",
        "        DDIM_ETA = 0.77 \n",
        "        settings[\"ddim_eta\"] = DDIM_ETA \n",
        "      PRECISION = \"autocast\" \n",
        "      settings[\"precision\"] = PRECISION\n",
        "      settings[\"width\"] = WIDTH\n",
        "      settings[\"height\"] = HEIGHT\n",
        "      settings[\"scale\"] = SCALE\n",
        "      IMG2IMG_POSTPROCESS = True \n",
        "      settings[\"img2img_postprocess\"] = IMG2IMG_POSTPROCESS\n",
        "      \n",
        " \n",
        "      if MODE == \"PROMPT FILE\":\n",
        "        FILE_LOCATION = \"/content/diffusers_output/1663720628_prompt.json\" \n",
        "        settings['prompt_file'] = FILE_LOCATION\n",
        "\n",
        "      if MODE == \"PROMPT\":\n",
        "\n",
        "        settings[\"prompt_type\"] = \"TEXT\"\n",
        "\n",
        "        if settings[\"prompt_type\"] == \"TEXT\":\n",
        "          TEXT_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:\"string\"}\n",
        "          settings[\"text_prompt\"] = TEXT_PROMPT\n",
        "\n",
        "        STEPS = 200 \n",
        "        settings[\"steps\"] = STEPS \n",
        "\n",
        "\n",
        "      elif MODE == \"CLIP GUIDED PROMPT\":\n",
        "        \n",
        "        CLIP_MODEL_ID = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\" \n",
        "        settings[\"clip_model_id\"] = CLIP_MODEL_ID\n",
        "        CLIP_MODE_TEXT_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\"\n",
        "        settings[\"text_prompt\"] = CLIP_MODE_TEXT_PROMPT\n",
        "        CLIP_GUIDANCE_PROMPT = \"\" \n",
        "        settings[\"clip_prompt\"] = CLIP_GUIDANCE_PROMPT\n",
        "        CLIP_MODE_STEPS = 200\n",
        "        settings[\"steps\"] = CLIP_MODE_STEPS\n",
        "        CLIP_MODE_SCALE = 13.7 \n",
        "        settings[\"scale\"] = CLIP_MODE_SCALE\n",
        "        CLIP_GUIDANCE_SCALE = 100 \n",
        "        settings[\"clip_guidance_scale\"] = CLIP_GUIDANCE_SCALE\n",
        "        CLIP_MODE_NUM_CUTOUTS = 4 \n",
        "        settings[\"clip_cutouts\"] = CLIP_MODE_NUM_CUTOUTS\n",
        "        CLIP_UNFREEZE_UNET = True \n",
        "        settings[\"unfreeze_unet\"] = CLIP_UNFREEZE_UNET\n",
        "        CLIP_UNFREEZE_VAE = True \n",
        "        settings[\"unfreeze_vae\"] = CLIP_UNFREEZE_VAE\n",
        "\n",
        "    \n",
        "      elif MODE == \"Inpainting\":\n",
        "\n",
        "        INPAINT_PROMPT = \"A cat sitting on a bench\" \n",
        "        settings[\"text_prompt\"] = INPAINT_PROMPT\n",
        "        \n",
        "        INPAINT_IMAGE = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\" \n",
        "        settings[\"inpaint_image\"] = INPAINT_IMAGE\n",
        "        \n",
        "        MASK_IMAGE = \"\" \n",
        "        settings[\"mask_image\"] = MASK_IMAGE\n",
        "        \n",
        "        INPAINT_STRENGTH = 0.5\n",
        "        settings[\"inpaint_strength\"] = INPAINT_STRENGTH\n",
        "\n",
        "\n",
        "      if MODE == \"IMG2IMG\" or settings[\"img2img_postprocess\"]:\n",
        "\n",
        "        IMG_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\"\n",
        "        \n",
        "        \n",
        "        INIT_IMAGE = \"https://raw.githubusercontent.com/dblunk88/txt2imghd/master/character_with_hat.jpg\" \n",
        "        settings[\"init_image\"] = INIT_IMAGE\n",
        "        \n",
        "        INIT_STRENGTH = 0.6 \n",
        "        \n",
        "        if settings[\"img2img_postprocess\"]:\n",
        "          settings['img2img'] = {}\n",
        "          settings['img2img'][\"text_prompt\"] = settings[\"text_prompt\"]\n",
        "          settings['img2img'][\"init_strength\"] = INIT_STRENGTH\n",
        "        else:\n",
        "          settings[\"text_prompt\"] = IMG_PROMPT\n",
        "          settings[\"init_strength\"] = INIT_STRENGTH\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "      IMAGE_UPSCALER = \"CodeFormer + Enhanced ESRGAN\" \n",
        "      settings[\"image_upscaler\"] = IMAGE_UPSCALER \n",
        "\n",
        "      UPSCALE_AMOUNT = 2 \n",
        "      settings[\"upscale_amount\"] = UPSCALE_AMOUNT \n",
        "\n",
        "      FIDELITY = 0.8 \n",
        "      settings[\"codeformer_fidelity\"] = FIDELITY\n",
        "\n",
        "      SHARPEN_AMOUNT = 1 \n",
        "      settings[\"sharpen_amount\"] = SHARPEN_AMOUNT\n",
        "\n",
        "\n",
        "      MODEL_ID = \"CompVis/stable-diffusion-v1-4\"\n",
        "      settings[\"model_id\"] = MODEL_ID\n",
        "\n",
        "      DIFFUSERS_VERSION = 'latest'\n",
        "      settings[\"diffusers_version\"] = DIFFUSERS_VERSION\n",
        "\n",
        "      CLEAN_PREVIEW_AFTER_ITERS = 19 \n",
        "      settings[\"clean_iters\"] = CLEAN_PREVIEW_AFTER_ITERS\n",
        "\n",
        "      SKIP_BULKY_PREVIEWS = False \n",
        "      settings[\"bulky_skip\"] = SKIP_BULKY_PREVIEWS\n",
        "\n",
        "      KEEP_SEED = False \n",
        "      settings[\"keep_seed\"] = KEEP_SEED\n",
        "\n",
        "      AMOUNT_OF_IMAGES = 16 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "      settings[\"num_iters\"] = AMOUNT_OF_IMAGES\n",
        "\n",
        "      RUN_FOREVER = False \n",
        "      settings[\"run_forever\"] = RUN_FOREVER\n",
        "\n",
        "      SAVE_PROMPT_DETAILS = True \n",
        "      settings[\"save_prompt_details\"] = SAVE_PROMPT_DETAILS\n",
        "\n",
        "      USE_DRIVE_FOR_PICS = False \n",
        "      settings[\"use_drive_for_pics\"] = USE_DRIVE_FOR_PICS\n",
        "\n",
        "      \n",
        "\n",
        "      if USE_DRIVE_FOR_PICS:\n",
        "        DRIVE_PIC_DIR = \"AI_PICS\" \n",
        "        settings[\"drive_pic_dir\"] = DRIVE_PIC_DIR\n",
        "\n",
        "      DELETE_ORIGINALS = True\n",
        "      settings[\"delete_originals\"] = DELETE_ORIGINALS\n",
        "\n",
        "      LOW_VRAM_PATCH = True \n",
        "      settings[\"low_vram_patch\"] = LOW_VRAM_PATCH\n",
        "\n",
        "      VRAM_OVER_SPEED = True \n",
        "      settings[\"vram_over_speed\"] = VRAM_OVER_SPEED\n",
        "\n",
        "      ENABLE_NSFW_FILTER = False \n",
        "      settings[\"enable_nsfw_filter\"] = ENABLE_NSFW_FILTER\n",
        "\n",
        "      return settings\n",
        "\n",
        "class Upscalers:\n",
        "\n",
        "  def check_upscalers(settings,image):\n",
        "    Cleaner.clean_env()\n",
        "    if settings['image_upscaler'] == 'GFPGAN':\n",
        "      image = Upscalers.gfpgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'Enhanced Real-ESRGAN':\n",
        "      image = Upscalers.esrgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'GFPGAN + Enhanced ESRGAN':\n",
        "      image = Upscalers.gfpgan_esrgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'CodeFormer':\n",
        "      image = Upscalers.codeformer(settings, image)\n",
        "    elif settings['image_upscaler'] == 'CodeFormer + Enhanced ESRGAN':\n",
        "      image = Upscalers.codeformer_esrgan(settings, image)\n",
        "    Cleaner.clean_env()\n",
        "    return image\n",
        "\n",
        "  def gfpgan(settings, image):\n",
        "    import os, subprocess\n",
        "    if not os.path.exists('/content/GFPGAN/'):\n",
        "      Upscalers.Install.gfpgan()\n",
        "    os.chdir('/content/GFPGAN/')\n",
        "    print(subprocess.run(['mkdir', f'/content/GFPGAN/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['mkdir', f'/content/GFPGAN/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    image.save('/content/GFPGAN/temp/temp.png')\n",
        "    print(subprocess.run(['python',f'/content/GFPGAN/inference_gfpgan.py','-i', '/content/GFPGAN/temp/temp.png','-o','/content/GFPGAN/results/', '-w', f'{settings[\"codeformer_fidelity\"]}','-s',f'{settings[\"upscale_amount\"]}', '--bg_upsampler', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    import PIL.Image\n",
        "    image = PIL.Image.open(f'/content/GFPGAN/results/restored_imgs/temp.png')\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/GFPGAN/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/GFPGAN/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/')\n",
        "    return image\n",
        "\n",
        "  def esrgan(settings, image):\n",
        "    def closest_value(input_list, input_value):\n",
        "      difference = lambda input_list : abs(input_list - input_value)\n",
        "      res = min(input_list, key=difference)\n",
        "      return res\n",
        "    import os\n",
        "    if int(settings[\"upscale_amount\"]) not in [2,4,8]:\n",
        "      nearest_value = closest_value([2,4,8],settings[\"upscale_amount\"])\n",
        "      settings[\"upscale_amount\"] = nearest_value\n",
        "      print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "    if not os.path.exists('/content/Real-ESRGAN'):\n",
        "      Upscalers.Install.esrgan()\n",
        "    if not os.path.exists(f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth'):\n",
        "      import subprocess\n",
        "      print(subprocess.run(['wget',f'https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x{settings[\"upscale_amount\"]}.pth','-O',f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/Real-ESRGAN')\n",
        "    from realesrgan import RealESRGAN\n",
        "    import torch\n",
        "    model = RealESRGAN(torch.device('cuda'), scale = settings[\"upscale_amount\"])\n",
        "    model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth')\n",
        "    import numpy as np\n",
        "    image = model.predict(np.array(image))\n",
        "    os.chdir('/content/')\n",
        "    model = None\n",
        "    Cleaner.clean_env()\n",
        "    return image\n",
        "\n",
        "  def codeformer(settings, image):\n",
        "    import os, subprocess\n",
        "    if not os.path.exists('/content/CodeFormer/'):\n",
        "      Upscalers.Install.codeformer()\n",
        "    print(subprocess.run(['mkdir', f'/content/CodeFormer/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['mkdir', f'/content/CodeFormer/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    image.save('/content/CodeFormer/temp/temp.png')\n",
        "    os.chdir('/content/CodeFormer/')\n",
        "    print(subprocess.run(['python',f'inference_codeformer.py','--w', f'{settings[\"codeformer_fidelity\"]}','--test_path','/content/CodeFormer/temp','--upscale',f'{settings[\"upscale_amount\"]}', '--bg_upsampler', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    import PIL.Image\n",
        "    image = PIL.Image.open(f'/content/CodeFormer/results/temp_{settings[\"codeformer_fidelity\"]}/final_results/temp.png')\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/CodeFormer/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/CodeFormer/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/')\n",
        "    return image\n",
        "\n",
        "  def gfpgan_esrgan(settings, image):\n",
        "    orig_upscale = settings['upscale_amount']\n",
        "    settings['upscale_amount'] = 1\n",
        "    image = Upscalers.gfpgan(settings, image)\n",
        "    settings['upscale_amount'] = orig_upscale\n",
        "    image = Upscalers.esrgan(settings, image)\n",
        "    return image\n",
        "\n",
        "  def codeformer_esrgan(settings, image):\n",
        "    orig_upscale = settings['upscale_amount']\n",
        "    settings['upscale_amount'] = 1\n",
        "    image = Upscalers.codeformer(settings, image)\n",
        "    settings['upscale_amount'] = orig_upscale\n",
        "    image = Upscalers.esrgan(settings, image)\n",
        "    return image\n",
        "    \n",
        "\n",
        "  class Install:\n",
        "\n",
        "    def gfpgan():\n",
        "      import subprocess\n",
        "      print(subprocess.run(['git','clone','https://github.com/TencentARC/GFPGAN.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','basicsr'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','facexlib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      import os\n",
        "      os.chdir('/content/GFPGAN')\n",
        "      print(subprocess.run(['pip','install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['wget','https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth', '-P', '/content/GFPGAN/experiments/pretrained_models'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "\n",
        "    def esrgan():\n",
        "      import subprocess, os\n",
        "      print(subprocess.run(['git','clone','https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('Real-ESRGAN')\n",
        "      print(subprocess.run(['git','reset', '--hard','2a5afd04a0e43956d1640db00d3a528ca5972fd2'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','-r','/content/Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "      \n",
        "    def codeformer():\n",
        "      import subprocess\n",
        "      import os\n",
        "      print(subprocess.run(['git','clone','https://github.com/sczhou/CodeFormer.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','-r','/content/CodeFormer/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/CodeFormer/')\n",
        "      print(subprocess.run(['python','basicsr/setup.py','develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','scripts/download_pretrained_models.py','facelib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','scripts/download_pretrained_models.py','CodeFormer'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "\n",
        "class Cache:\n",
        "  class Pipe:\n",
        "    def __init__(self, settings):\n",
        "      global pipe\n",
        "      global pipetype\n",
        "      try:\n",
        "        if pipetype != settings[\"mode\"] or pipe is None:\n",
        "          self.make(settings)\n",
        "      except NameError:\n",
        "        self.make(settings)\n",
        "      Manager.Diffusion.Scheduler.make(settings)\n",
        "      self.pipe = pipe\n",
        "      self.pipetype = settings_pipetype\n",
        "\n",
        "    def make(settings):\n",
        "      global pipe \n",
        "      global pipetype\n",
        "      pipe = None\n",
        "      Cleaner.clean_env()\n",
        "      pipetype = settings['mode']\n",
        "      pipe_library = Manager.manage_imports(pipetype)\n",
        "      import os, subprocess, torch\n",
        "      username, token = Manager.Diffusion.creds()\n",
        "      subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "      left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "      right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      print('Making Pipe')\n",
        "      if settings['mode'] == \"CLIP GUIDED PROMPT\":\n",
        "        import torch\n",
        "        from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
        "        from PIL import Image\n",
        "        from transformers import CLIPFeatureExtractor, CLIPModel\n",
        "        import CLIP_GUIDED\n",
        "        def create_clip_guided_pipeline(\n",
        "            model_id=\"CompVis/stable-diffusion-v1-4\", clip_model_id=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", scheduler=\"plms\", low_vram_patch=True\n",
        "        ):\n",
        "            if low_vram_patch:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                  model_id,\n",
        "                  torch_dtype=torch.float16,\n",
        "                  revision=\"fp16\",\n",
        "                  use_auth_token=True,\n",
        "              )\n",
        "              clip_model = CLIPModel.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "              feature_extractor = CLIPFeatureExtractor.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "            else:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                  model_id,\n",
        "                  use_auth_token=True,\n",
        "              )\n",
        "              clip_model = CLIPModel.from_pretrained(clip_model_id)\n",
        "              feature_extractor = CLIPFeatureExtractor.from_pretrained(clip_model_id)\n",
        "\n",
        "            schedulers = Manager.manage_imports(settings[\"scheduler\"])\n",
        "            if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "            elif settings[\"scheduler\"] == 'k-lms':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "            elif settings[\"scheduler\"] == 'ddim':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "            \n",
        "\n",
        "            guided_pipeline = CLIP_GUIDED.CLIPGuidedStableDiffusion(\n",
        "                unet=pipeline.unet,\n",
        "                vae=pipeline.vae,\n",
        "                tokenizer=pipeline.tokenizer,\n",
        "                text_encoder=pipeline.text_encoder,\n",
        "                scheduler=scheduler,\n",
        "                clip_model=clip_model,\n",
        "                feature_extractor=feature_extractor,\n",
        "            )\n",
        "\n",
        "            return guided_pipeline\n",
        "        \n",
        "        local_pipe = create_clip_guided_pipeline(settings[\"model_id\"], settings[\"clip_model_id\"], settings['scheduler'], settings[\"low_vram_patch\"])\n",
        "        local_pipe = local_pipe.to(\"cuda\")\n",
        "      else:\n",
        "        if settings[\"low_vram_patch\"]:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "          if settings[\"mode\"] == \"PROMPT\":\n",
        "            del local_pipe.vae.encoder\n",
        "        else:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "      pipe = local_pipe\n",
        "      local_pipe = None\n",
        "      Cleaner.clean_env()\n",
        "\n",
        "\n",
        "class Manager:\n",
        "  def __init__(self):\n",
        "    self.colab = Colab()\n",
        "    self.cache = Cache()\n",
        "    from IPython.display import Javascript\n",
        "    display(Javascript(\"google.colab.output.resizeIframeToContent()\"))\n",
        "\n",
        "  def manage_imports(requester):\n",
        "    if requester == 'manage_drive':\n",
        "      from os.path import exists\n",
        "      from os import makedirs\n",
        "      from google.colab import drive\n",
        "      return exists, drive.mount, makedirs\n",
        "\n",
        "    elif requester == 'patch_nsfw':\n",
        "      from shutil import copyfile\n",
        "      from os import remove\n",
        "      return copyfile, remove\n",
        "\n",
        "    elif requester == 'general_diffusion_run':\n",
        "      import torch, sys\n",
        "      from random import randint\n",
        "      return torch, randint, sys\n",
        "\n",
        "    elif requester == \"prompt_run\":\n",
        "      pass\n",
        "\n",
        "    elif requester == \"img2img_run\":\n",
        "      pass\n",
        "    \n",
        "    elif requester == \"inpainter_run\":\n",
        "      pass\n",
        "    \n",
        "    elif requester == 'clean_env':\n",
        "      import gc, torch\n",
        "      return gc, torch\n",
        "\n",
        "    elif requester == \"PROMPT\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionPipeline\n",
        "      return StableDiffusionPipeline\n",
        "    \n",
        "    elif requester == \"IMG2IMG\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionImg2ImgPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionImg2ImgPipeline\n",
        "      return StableDiffusionImg2ImgPipeline\n",
        "\n",
        "    elif requester == \"Inpainting\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionInpaintPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionInpaintPipeline\n",
        "      return StableDiffusionInpaintPipeline\n",
        "\n",
        "    elif requester == \"CLIP GUIDED PROMPT\":\n",
        "      try: \n",
        "        from CLIP_GUIDED import CLIPGuidedStableDiffusion\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from CLIP_GUIDED import CLIPGuidedStableDiffusion\n",
        "      return CLIPGuidedStableDiffusion\n",
        "\n",
        "    elif requester == 'diffuser_install':\n",
        "      import subprocess, os\n",
        "      return subprocess, os\n",
        "\n",
        "    elif requester == 'default' or requester == 'pndm':\n",
        "      from diffusers.schedulers import PNDMScheduler\n",
        "      return PNDMScheduler\n",
        "    \n",
        "    elif requester == 'k-lms':\n",
        "      from diffusers.schedulers import LMSDiscreteScheduler\n",
        "      return LMSDiscreteScheduler\n",
        "\n",
        "    elif requester == 'ddim':\n",
        "      from diffusers.schedulers import DDIMScheduler\n",
        "      return DDIMScheduler\n",
        "\n",
        "\n",
        "\n",
        "  def eval_settings(self):\n",
        "    settings = self.colab.settings\n",
        "    if settings['mode'] == \"PROMPT FILE\":\n",
        "      with open(settings['prompt_file'],'r') as file:\n",
        "        import json\n",
        "        self.colab.settings = json.loads(file.read())\n",
        "        settings = self.colab.settings\n",
        "    import json\n",
        "    print(json.dumps(settings, indent=2))\n",
        "    global pipetype\n",
        "    global pipe\n",
        "    try:\n",
        "      if pipetype != settings['mode'] or pipe is None:\n",
        "        Cache.Pipe.make(settings)\n",
        "    except NameError:\n",
        "      Cache.Pipe.make(settings)\n",
        "    if settings[\"use_drive_for_pics\"]:\n",
        "      Colab.manage_drive(settings['drive_pic_dir'])\n",
        "    \n",
        "\n",
        "  class Diffusion:\n",
        "    def patch_nsfw(ENABLE_NSFW_FILTER):\n",
        "      copyfile, remove = Manager.manage_imports('patch_nsfw')\n",
        "      remove('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      if ENABLE_NSFW_FILTER:\n",
        "        copyfile(f'/content/safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      else:\n",
        "        copyfile(f'/content/safety_checker_patched.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "\n",
        "    def install_diffusers():\n",
        "      subprocess, os = Manager.manage_imports('diffuser_install')\n",
        "      settings = Colab.UserSettings.set_settings()\n",
        "      \n",
        "      print('Installing Transformers')\n",
        "      print(subprocess.run(['pip','install','transformers'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Installing Diffusers')\n",
        "      if settings['diffusers_version'] == 'latest':\n",
        "        print(subprocess.run(['pip','install','-U',f'git+https://github.com/huggingface/diffusers.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      else:\n",
        "        print(subprocess.run(['pip','install','-U',f'git+https://github.com/huggingface/diffusers.git@{settings[\"diffusers_version\"]}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Creating Directories')\n",
        "      print(subprocess.run(['mkdir','diffusers_output'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Installing Dependencies')\n",
        "      print(subprocess.run(['pip','install','pytorch-pretrained-bert','spacy','ftfy','scipy'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Populating Spacy')\n",
        "      print(subprocess.run(['python','-m','space','download','en'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Logging into Huggingface')\n",
        "      username, token = Manager.Diffusion.creds()\n",
        "      subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "      left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "      right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      # Manager.Diffusion.install_model(username, token, settings[\"model_id\"])\n",
        "      Colab.clear()\n",
        "      if not os.path.exists('/content/safety_checker_patched.py'):\n",
        "        print('Creating Patches')\n",
        "        print(subprocess.run(['cp','/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','/content/safety_checker.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['cp','/content/safety_checker.py','/content/safety_checker_patched.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        with open(f'/content/safety_checker_patched.py','r') as unpatched_file:\n",
        "          patch = unpatched_file.read().replace('for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):','#for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):').replace('if has_nsfw_concept:','# if has_nsfw_concept:').replace('images[idx] = np.zeros(images[idx].shape)  # black image', '# images[idx] = np.zeros(images[idx].shape)  # black image').replace(\"Potential NSFW content was detected in one or more images. A black image will be returned instead.\",\"Potential NSFW content was detected in one or more images. It's patched out, no actions were taken.\").replace(\" Try again with a different prompt and/or seed.\",\"\")\n",
        "        with open(f'/content/safety_checker_patched.py','w') as file:\n",
        "          file.write(patch)\n",
        "      with open('/content/CLIP_GUIDED.py', 'w') as file:\n",
        "        file.write('''\n",
        "import inspect\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from diffusers import AutoencoderKL, DiffusionPipeline, LMSDiscreteScheduler, PNDMScheduler, UNet2DConditionModel\n",
        "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipelineOutput\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPFeatureExtractor, CLIPModel, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cut_power=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cut_size = cut_size\n",
        "        self.cut_power = cut_power\n",
        "\n",
        "    def forward(self, pixel_values, num_cutouts):\n",
        "        sideY, sideX = pixel_values.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(num_cutouts):\n",
        "            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def set_requires_grad(model, value):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = value\n",
        "\n",
        "\n",
        "class CLIPGuidedStableDiffusion(DiffusionPipeline):\n",
        "    \"\"\"CLIP guided stable diffusion based on the amazing repo by @crowsonkb and @Jack000\n",
        "    - https://github.com/Jack000/glid-3-xl\n",
        "    - https://github.dev/crowsonkb/k-diffusion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        clip_model: CLIPModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler],\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            clip_model=clip_model,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "        self.normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "        self.make_cutouts = MakeCutouts(feature_extractor.size)\n",
        "\n",
        "        set_requires_grad(self.text_encoder, False)\n",
        "        set_requires_grad(self.clip_model, False)\n",
        "\n",
        "    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n",
        "        if slice_size == \"auto\":\n",
        "            # half the attention head size is usually a good trade-off between\n",
        "            # speed and memory\n",
        "            slice_size = self.unet.config.attention_head_dim // 2\n",
        "        self.unet.set_attention_slice(slice_size)\n",
        "\n",
        "    def disable_attention_slicing(self):\n",
        "        self.enable_attention_slicing(None)\n",
        "\n",
        "    def freeze_vae(self):\n",
        "        set_requires_grad(self.vae, False)\n",
        "\n",
        "    def unfreeze_vae(self):\n",
        "        set_requires_grad(self.vae, True)\n",
        "\n",
        "    def freeze_unet(self):\n",
        "        set_requires_grad(self.unet, False)\n",
        "\n",
        "    def unfreeze_unet(self):\n",
        "        set_requires_grad(self.unet, True)\n",
        "\n",
        "    @torch.enable_grad()\n",
        "    def cond_fn(\n",
        "        self,\n",
        "        latents,\n",
        "        timestep,\n",
        "        index,\n",
        "        text_embeddings,\n",
        "        noise_pred_original,\n",
        "        text_embeddings_clip,\n",
        "        clip_guidance_scale,\n",
        "        num_cutouts,\n",
        "        use_cutouts=True,\n",
        "    ):\n",
        "        latents = latents.detach().requires_grad_()\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "            latent_model_input = latents / ((sigma**2 + 1) ** 0.5)\n",
        "        else:\n",
        "            latent_model_input = latents\n",
        "\n",
        "        # predict the noise residual\n",
        "        noise_pred = self.unet(latent_model_input, timestep, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "        if isinstance(self.scheduler, PNDMScheduler):\n",
        "            alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
        "            beta_prod_t = 1 - alpha_prod_t\n",
        "            # compute predicted original sample from predicted noise also called\n",
        "            # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "            pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n",
        "\n",
        "            fac = torch.sqrt(beta_prod_t)\n",
        "            sample = pred_original_sample * (fac) + latents * (1 - fac)\n",
        "        elif isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            sample = latents - sigma * noise_pred\n",
        "        else:\n",
        "            raise ValueError(f\"scheduler type {type(self.scheduler)} not supported\")\n",
        "\n",
        "        sample = 1 / 0.18215 * sample\n",
        "        image = self.vae.decode(sample).sample\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        if use_cutouts:\n",
        "            image = self.make_cutouts(image, num_cutouts)\n",
        "        else:\n",
        "            image = transforms.Resize(self.feature_extractor.size)(image)\n",
        "        image = self.normalize(image)\n",
        "\n",
        "        image_embeddings_clip = self.clip_model.get_image_features(image).float()\n",
        "        image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        if use_cutouts:\n",
        "            dists = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip)\n",
        "            dists = dists.view([num_cutouts, sample.shape[0], -1])\n",
        "            loss = dists.sum(2).mean(0).sum() * clip_guidance_scale\n",
        "        else:\n",
        "            loss = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip).mean() * clip_guidance_scale\n",
        "\n",
        "        grads = -torch.autograd.grad(loss, latents)[0]\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents.detach() + grads * (sigma**2)\n",
        "            noise_pred = noise_pred_original\n",
        "        else:\n",
        "            noise_pred = noise_pred_original - torch.sqrt(beta_prod_t) * grads\n",
        "        return noise_pred, latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        height: Optional[int] = 512,\n",
        "        width: Optional[int] = 512,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        clip_guidance_scale: Optional[float] = 100,\n",
        "        clip_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_cutouts: Optional[int] = 4,\n",
        "        use_cutouts: Optional[bool] = True,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "    ):\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        if clip_guidance_scale > 0:\n",
        "            if clip_prompt is not None:\n",
        "                clip_text_input = self.tokenizer(\n",
        "                    clip_prompt,\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.tokenizer.model_max_length,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\",\n",
        "                ).input_ids.to(self.device)\n",
        "            else:\n",
        "                clip_text_input = text_input.input_ids.to(self.device)\n",
        "            text_embeddings_clip = self.clip_model.get_text_features(clip_text_input)\n",
        "            text_embeddings_clip = text_embeddings_clip / text_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # get the initial random noise unless the user supplied it\n",
        "\n",
        "        # Unlike in other pipelines, latents need to be generated in the target device\n",
        "        # for 1-to-1 results reproducibility with the CompVis implementation.\n",
        "        # However this currently doesn't work in `mps`.\n",
        "        latents_device = \"cpu\" if self.device.type == \"mps\" else self.device\n",
        "        latents_shape = (batch_size, self.unet.in_channels, height // 8, width // 8)\n",
        "        if latents is None:\n",
        "            latents = torch.randn(\n",
        "                latents_shape,\n",
        "                generator=generator,\n",
        "                device=latents_device,\n",
        "            )\n",
        "        else:\n",
        "            if latents.shape != latents_shape:\n",
        "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
        "        latents = latents.to(self.device)\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        if accepts_offset:\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # if we use LMSDiscreteScheduler, let's make sure latents are multiplied by sigmas\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "        for i, t in enumerate(self.progress_bar(self.scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                sigma = self.scheduler.sigmas[i]\n",
        "                # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "            # perform classifier free guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # perform clip guidance\n",
        "            if clip_guidance_scale > 0:\n",
        "                text_embeddings_for_guidance = (\n",
        "                    text_embeddings.chunk(2)[0] if do_classifier_free_guidance else text_embeddings\n",
        "                )\n",
        "                noise_pred, latents = self.cond_fn(\n",
        "                    latents,\n",
        "                    t,\n",
        "                    i,\n",
        "                    text_embeddings_for_guidance,\n",
        "                    noise_pred,\n",
        "                    text_embeddings_clip,\n",
        "                    clip_guidance_scale,\n",
        "                    num_cutouts,\n",
        "                    use_cutouts,\n",
        "                )\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                latents = self.scheduler.step(noise_pred, i, latents).prev_sample\n",
        "            else:\n",
        "                latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image, None)\n",
        "\n",
        "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
        "\n",
        "        ''')\n",
        "      Manager.Diffusion.patch_nsfw(settings['enable_nsfw_filter'])\n",
        "      Colab.clear()\n",
        "\n",
        "\n",
        "    def install_model(username, token, model_id):\n",
        "      subprocess, os = Manager.manage_imports('diffuser_install')\n",
        "      print('Installing Model')\n",
        "      print(subprocess.run(['git','lfs','install'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      # This will take a while\n",
        "      print(subprocess.run(['git','lfs','clone',f'https://{username}:{token}@huggingface.co/{model_id}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "    def creds():\n",
        "      return 'x90', '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'\n",
        "\n",
        "    def img2img_init(settings):\n",
        "      def preprocess(image):\n",
        "        import numpy as np\n",
        "        import torch\n",
        "        import PIL.Image\n",
        "        w, h = image.size\n",
        "        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "        image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "        image = np.array(image).astype(np.float32) / 255.0\n",
        "        image = image[None].transpose(0, 3, 1, 2)\n",
        "        image = torch.from_numpy(image)\n",
        "        return 2.*image - 1.\n",
        "      global pipe\n",
        "      import PIL.Image\n",
        "      import requests\n",
        "      if 'http' in settings['init_image']:\n",
        "        from io import BytesIO\n",
        "        response = requests.get(settings['init_image'])\n",
        "        init_image = PIL.Image.open(BytesIO(response.content))\n",
        "      elif settings['init_image'] is None or settings['init_image'] == \"\":\n",
        "        init_image = Colab.Images.Painter.img2img(settings['width'], settings['height'])\n",
        "      else:\n",
        "        #it's a file\n",
        "        init_image = PIL.Image.open(settings['init_image'])\n",
        "      print(\"Init Image (automatically resized to match user input)\")\n",
        "      init_image = init_image.resize((settings['width'], settings['height']))\n",
        "      display(init_image)\n",
        "      init_image = preprocess(init_image.convert(\"RGB\"))\n",
        "      return init_image\n",
        "    \n",
        "    def inpaint_init(settings):\n",
        "      import PIL.Image\n",
        "      def download(location):\n",
        "        from io import BytesIO\n",
        "        import requests\n",
        "        import PIL.Image\n",
        "        response = requests.get(location)\n",
        "        return PIL.Image.open(BytesIO(response.content))\n",
        "      if 'http' in settings[\"inpaint_image\"]:\n",
        "        init_image = download(settings[\"inpaint_image\"])\n",
        "      else:\n",
        "        init_image = PIL.Image.open(settings[\"inpaint_image\"])\n",
        "      init_image = init_image.resize((settings['width'], settings['height']))\n",
        "      if 'http' in settings[\"mask_image\"]:\n",
        "        mask_image = download(settings[\"mask_image\"])\n",
        "      elif settings[\"mask_image\"]:\n",
        "        mask_image = PIL.Image.open(settings[\"mask_image\"])\n",
        "      else:\n",
        "        init_image.save(\"init.jpg\")\n",
        "        mask_image = Colab.Images.Painter.inpaint(settings['width'], settings['height'])\n",
        "      mask_image.resize((settings['width'], settings['height']))\n",
        "      return init_image, mask_image\n",
        "\n",
        "    class Scheduler:\n",
        "      def make(settings):\n",
        "        scheduler = Manager.manage_imports(settings[\"scheduler\"])\n",
        "        global pipe\n",
        "        if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "        elif settings[\"scheduler\"] == 'k-lms':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif settings[\"scheduler\"] == 'ddim':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "    class Runner:\n",
        "\n",
        "      def run(settings):\n",
        "        def sharpen_mage(image, samples=1):\n",
        "          from PIL import ImageFilter\n",
        "          im = image\n",
        "          for i in range(samples):\n",
        "              im = im.filter(ImageFilter.SHARPEN)\n",
        "          return im\n",
        "        import time\n",
        "        torch, precision_scope, randint, sys = Manager.Diffusion.Runner.get_general_imports(settings)\n",
        "        with torch.no_grad():\n",
        "          with precision_scope(\"cuda\"):\n",
        "            if settings['seed'] == 0:\n",
        "              settings['seed'] = randint(0,sys.maxsize)\n",
        "            generator = torch.Generator(\"cuda\").manual_seed(settings['seed'])\n",
        "            counter = 1\n",
        "            clean_counter = 0\n",
        "            running = True\n",
        "            if settings[\"use_drive_for_pics\"]:\n",
        "              outdir = f'/content/drive/MyDrive/{settings[\"drive_pic_dir\"]}'\n",
        "            else:\n",
        "              outdir = '/content/diffusers_output'\n",
        "            epoch_time = int(time.time())\n",
        "            if settings[\"save_prompt_details\"]:\n",
        "              with open(f'{outdir}/{epoch_time}_prompt.json', 'w') as file:\n",
        "                import json\n",
        "                file.write(json.dumps(settings, indent=2))\n",
        "            if settings['mode'] == \"IMG2IMG\":\n",
        "              init_image = Manager.Diffusion.img2img_init(settings)\n",
        "            elif settings['mode'] == 'Inpainting':\n",
        "              init_image, mask_image = Manager.Diffusion.inpaint_init(settings)\n",
        "            while running:\n",
        "              Cleaner.clean_env()\n",
        "              if settings[\"mode\"] == \"PROMPT\":\n",
        "                if settings['prompt_type'] == 'TEXT':\n",
        "                  image = Manager.Diffusion.Runner.text_prompt(settings, torch, generator)\n",
        "                else:\n",
        "                  for prompt in settings[\"file_prompt\"]:\n",
        "                    pass\n",
        "                    # TODO\n",
        "              elif settings[\"mode\"] == \"IMG2IMG\":\n",
        "                image = Manager.Diffusion.Runner.img_to_img(settings, torch, generator, init_image)\n",
        "              elif settings[\"mode\"] == \"Inpainting\":\n",
        "                image = Manager.Diffusion.Runner.inpainting(settings, torch, generator, init_image, mask_image)\n",
        "              elif settings[\"mode\"] == \"CLIP GUIDED PROMPT\":\n",
        "                Cleaner.clean_env()\n",
        "                image = Manager.Diffusion.Runner.clip_guided_prompt(settings, torch, generator)\n",
        "                Cleaner.clean_env()\n",
        "              if settings['image_upscaler'] == 'None' or not settings[\"delete_originals\"]:\n",
        "                image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_original.png')\n",
        "                epoch_time = int(time.time())\n",
        "              clean_counter += 1\n",
        "              if settings[\"clean_iters\"] <= clean_counter:\n",
        "                Colab.clear()\n",
        "                clean_counter = 0\n",
        "              print(f'Image {counter}. SEED: {settings[\"seed\"]}')\n",
        "              display(image)\n",
        "              print('Enhancing and Upscaling')\n",
        "              if settings['image_upscaler'] != 'None':\n",
        "                image = Upscalers.check_upscalers(settings,image)\n",
        "                if settings['sharpen_amount'] > 0:\n",
        "                  image = sharpen_mage(image, settings['sharpen_amount'])\n",
        "                  if not settings[\"bulky_skip\"]:\n",
        "                    display(image)\n",
        "                  image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}_sharpened_{settings[\"sharpen_amount\"]}.png')\n",
        "                  epoch_time = int(time.time())\n",
        "                else:\n",
        "                  if not settings[\"bulky_skip\"]:\n",
        "                    display(image)\n",
        "                  image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}.png')\n",
        "                  epoch_time = int(time.time())\n",
        "              if settings[\"img2img_postprocess\"]:\n",
        "                image = Manager.Diffusion.Runner.img2img_postprocess(settings, image, generator)\n",
        "                if not settings[\"bulky_skip\"]:\n",
        "                  display(image)\n",
        "                  image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}_img2imgenhanced.png')\n",
        "              if not settings['keep_seed']:\n",
        "                settings['seed'] += 1\n",
        "                generator = torch.Generator(\"cuda\").manual_seed(settings['seed'])\n",
        "\n",
        "              if not settings['run_forever']:\n",
        "                if counter >= settings['num_iters']:\n",
        "                  running = False\n",
        "                  \n",
        "              counter += 1\n",
        "\n",
        "      def img2img_postprocess(settings, image, generator):\n",
        "        import torch\n",
        "        print(\"running img2img postprocessing. Switching to img2img pipe\")\n",
        "        global pipe\n",
        "        pipe = None\n",
        "        Cleaner.clean_env()\n",
        "        pipe_library = Manager.manage_imports(\"IMG2IMG\")\n",
        "        import os, subprocess, torch\n",
        "        username, token = Manager.Diffusion.creds()\n",
        "        subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "        left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "        right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if settings[\"low_vram_patch\"]:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "        else:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "        pipe = local_pipe\n",
        "        local_pipe = None\n",
        "        Cleaner.clean_env()\n",
        "        scheduler = Manager.manage_imports(settings[\"scheduler\"])\n",
        "        if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "        elif settings[\"scheduler\"] == 'k-lms':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif settings[\"scheduler\"] == 'ddim':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "        img2img_settings = {\n",
        "            \"text_prompt\":settings['img2img'][\"text_prompt\"],\n",
        "            'init_strength':settings['img2img'][\"init_strength\"],\n",
        "            'scheduler':settings['scheduler'],\n",
        "            'scale':settings['scale']\n",
        "        }\n",
        "        image = Manager.Diffusion.Runner.img_to_img(img2img_settings, torch, generator, image)\n",
        "        pipe = None\n",
        "        print('Switching back to old pipe and then displaying the image')\n",
        "        Cleaner.clean_env()\n",
        "        Cache.Pipe.make(settings)\n",
        "        Manager.Diffusion.Scheduler.make(settings)\n",
        "        return image\n",
        "\n",
        "      def get_general_imports(settings):\n",
        "        torch, randint, sys = Manager.manage_imports('general_diffusion_run')\n",
        "        if settings['precision'] == 'autocast':\n",
        "          return torch, torch.autocast, randint, sys\n",
        "        else:\n",
        "          from contextlib import nullcontext\n",
        "          return torch, nullcontext, randint, sys\n",
        "\n",
        "      def text_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], width=settings['width'], height=settings['height'], guidance_scale=settings['scale'], eta=settings[\"ddim_eta\"], generator=generator)\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], width=settings['width'], height=settings['height'], guidance_scale=settings['scale'], generator=generator)\n",
        "        return image[\"sample\"][0]\n",
        "        \n",
        "\n",
        "      def file_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        pass\n",
        "\n",
        "      def img_to_img(settings, torch, generator, init_image):\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], init_image=init_image, strength=settings['init_strength'], eta=settings[\"ddim_eta\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], init_image=init_image, strength=settings['init_strength'], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        return image\n",
        "\n",
        "      def inpainting(settings, torch, generator, init_image, mask_image):\n",
        "        global pipe\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], init_image=init_image, mask_image=mask_image, strength=settings[\"inpaint_strength\"], eta=settings[\"ddim_eta\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], init_image=init_image, mask_image=mask_image, strength=settings[\"inpaint_strength\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        return image\n",
        "\n",
        "      def clip_guided_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        if settings[\"unfreeze_unet\"] == \"True\":\n",
        "          pipe.unfreeze_unet()\n",
        "        else:\n",
        "          pipe.freeze_unet()\n",
        "\n",
        "        if settings[\"unfreeze_vae\"] == \"True\":\n",
        "         pipe.unfreeze_vae()\n",
        "        else:\n",
        "          pipe.freeze_vae()\n",
        "        use_cutouts = False\n",
        "        if settings[\"clip_cutouts\"] >= 1:\n",
        "          use_cutouts = True\n",
        "        Cleaner.clean_env()\n",
        "        image = pipe(\n",
        "            settings[\"text_prompt\"],\n",
        "            clip_prompt=settings[\"clip_prompt\"] if settings[\"clip_prompt\"].strip() != \"\" else None,\n",
        "            num_inference_steps=settings[\"steps\"],\n",
        "            guidance_scale=settings[\"scale\"], \n",
        "            clip_guidance_scale=settings[\"clip_guidance_scale\"],\n",
        "            num_cutouts=settings[\"clip_cutouts\"],\n",
        "            use_cutouts=use_cutouts == \"True\",\n",
        "            generator=generator,\n",
        "            width=settings[\"width\"],\n",
        "            height=settings[\"height\"]\n",
        "        ).images[0]\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  Cleaner.clean_env()\n",
        "  manager = Manager()\n",
        "  # makes pipe and schedulers\n",
        "  manager.eval_settings()\n",
        "  # make images\n",
        "  Manager.Diffusion.Runner.run(manager.colab.settings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font color=\"orange\">**Clean Environment Up**</font>\n",
        "#@markdown If it errors out, click this\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FxvIfgi9YH4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "h-qNQtzBRbjo"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}